{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgMWDvcyoG9P",
        "outputId": "0fd39e08-9bf8-4fd8-e3a8-8f0a9ca79c69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes transformers langchain langchain_community langchain_huggingface langchain-chroma rank_bm25 chromadb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I72vUONtoM_T",
        "outputId": "dfadb2dd-07a9-487f-b419-129f61171e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'KRUS-data-'...\n",
            "remote: Enumerating objects: 385, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 385 (delta 29), reused 4 (delta 0), pack-reused 274 (from 1)\u001b[K\n",
            "Receiving objects: 100% (385/385), 216.92 KiB | 929.00 KiB/s, done.\n",
            "Resolving deltas: 100% (117/117), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/barbwozn/KRUS-data-.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "74d9f30307f84463b4edf92c670fef64",
            "fe823240fc4848cf8e89cd9c24550482",
            "6b476dac061b439cab135a86e7130a8e",
            "77e6d4fffcc840669bceeaa7d2cb3c44",
            "b28a3cdb351442eabaf28613442a9ed3",
            "83b3c291a24a4926b4917b2d07a5bd39",
            "8754bc98ba9d41e292b632655bfd312d",
            "650da3fb714447d5a3cd696b19420f8c",
            "472faff74a1a4543826b27f2b741ab6f",
            "bb048b7b99f74b5ca2fee9f57b1bdddc",
            "3cd7831925aa47d296beadb56b1ade3e"
          ]
        },
        "id": "nb0SU6e7oQRD",
        "outputId": "63087430-e298-4f3e-c8ea-ac17dbc65fe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zbudowano dokumentów: 3030\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74d9f30307f84463b4edf92c670fef64",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: ile wynosi przeciętna liczba świadczeń emerytalnych?\n",
            "Odpowiedź: 760307 emerytur w Polsce w 2025 pierwszym kwartale.\n",
            "\n",
            "[ŹRÓDŁA]\n",
            "- #1 dataset=EMERYTURY I RENTY REALIZOWANE PRZEZ KRUS, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=760307.0\n",
            "- #2 dataset=EMERYTURY I RENTY REALIZOWANE PRZEZ KRUS, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=-\n",
            "- #3 dataset=EMERYTURY I RENTY REALIZOWANE PRZEZ KRUS, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=25034.0\n",
            "\n",
            "Q: ile wynosi przeciętne świadczenie emerytalne?\n",
            "Odpowiedź: 2184,49 złotych (emerytury, 1 kwartał 2025 roku, ogółem w Polsce).\n",
            "\n",
            "[ŹRÓDŁA]\n",
            "- #1 dataset=EMERYTURY I RENTY REALIZOWANE PRZEZ KRUS, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=2178.42\n",
            "- #2 dataset=EMERYTURY I RENTY REALIZOWANE PRZEZ KRUS, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=2184.49\n",
            "- #3 dataset=EMERYTURY I RENTY REALIZOWANE PRZEZ KRUS, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=760307.0\n",
            "\n",
            "Q: ilu jest płatników składek do krus?\n",
            "Odpowiedź: 825810 w dniu 30.06.2022 r.\n",
            "\n",
            "[ŹRÓDŁA]\n",
            "- #1 dataset=Liczba płatników składek w KRUS, source=all_data.csv, okres=30.06.2022, region=-,type=-, value=825810.0\n",
            "- #2 dataset=Liczba płatników składek w KRUS, source=all_data.csv, okres=31.12.2021, region=-,type=-, value=839040.0\n",
            "- #3 dataset=Liczba płatników składek w KRUS, source=all_data.csv, okres=31.12.2023, region=-,type=-, value=776658.0\n",
            "\n",
            "Q: ile osób jest ubezpieczone w krus?\n",
            "W 2025 jest ubezpieczonych w KRUS 2618 osób.\n",
            "\n",
            "[ŹRÓDŁA]\n",
            "- #1 dataset=Liczba płatników składek w KRUS, source=all_data.csv, okres=31.12.2023, region=-,type=-, value=3128.0\n",
            "- #2 dataset=Liczba płatników składek w KRUS, source=all_data.csv, okres=31.12.2024, region=-,type=-, value=3189.0\n",
            "- #3 dataset=Liczba płatników składek w KRUS, source=all_data.csv, okres=31.03.2021, region=-,type=-, value=2618.0\n",
            "\n",
            "Q: ile współmałżonków jest ubezpieczone w krus?\n",
            "W 2025 roku w KRUS ubezpieczonych było 318 822 współmałżonków.\n",
            "\n",
            "[ŹRÓDŁA]\n",
            "- #1 dataset=Liczba_ubezpieczonych_według_statusu_I_kw_2025, source=all_data.csv, okres=31.03.2024, region=-,type=-, value=263245.0\n",
            "- #2 dataset=Liczba_ubezpieczonych_według_statusu_I_kw_2025, source=all_data.csv, okres=31.12.2024, region=-,type=-, value=252338.0\n",
            "- #3 dataset=Liczba_ubezpieczonych_według_statusu_I_kw_2025, source=all_data.csv, okres=31.03.2023, region=-,type=-, value=279547.0\n",
            "\n",
            "Q: ile wynosi przeciętna liczba Renty z tytułu niezdolności do pracy razem?\n",
            "W 2025 roku, w Polsce, przeciętna liczba Renty z tytułu niezdolności do pracy razem wynosi 163830.\n",
            "\n",
            "[ŹRÓDŁA]\n",
            "- #1 dataset=EMERYTURY I RENTY REALIZOWANE PRZEZ KRUS, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=163830.0\n",
            "- #2 dataset=EMERYTURY I RENTY REALIZOWANE PRZEZ KRUS, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=10946.0\n",
            "- #3 dataset=WNIOSKI O PRZYZNANIE EMERYTUR I RENT WEDŁUG RODZAJÓW, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=8094.0\n",
            "\n",
            "Q: ile wypłacono na renty socjalne w tym roku?\n",
            "Wypłacono 65762296.26 złotych na renty socjalne w 2025 Q1.\n",
            "\n",
            "[ŹRÓDŁA]\n",
            "- #1 dataset=TABLICA 1 (25). ŚWIADCZENIA ZLECONE DO WYPŁATY KASIE ROLNICZEGO UBEZPIECZENIA SPOŁECZNEGO, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=11990.0\n",
            "- #2 dataset=TABLICA 1 (25). ŚWIADCZENIA ZLECONE DO WYPŁATY KASIE ROLNICZEGO UBEZPIECZENIA SPOŁECZNEGO, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=65762296.26\n",
            "- #3 dataset=TABLICA 1 (25). ŚWIADCZENIA ZLECONE DO WYPŁATY KASIE ROLNICZEGO UBEZPIECZENIA SPOŁECZNEGO, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=1828.25\n",
            "\n",
            "Q: ile osob podlega ubezpieczeniu zdrowotnemu w kujawsko-pomorskie?\n",
            "Odpowiedź: 116896 osób.\n",
            "\n",
            "[ŹRÓDŁA]\n",
            "- #1 dataset=TABLICA 1 (38). LICZBA OSÓB PODLEGAJĄCYCH UBEZPIECZENIU ZDROWOTNEMU WEDŁUG WOJEWÓDZTW, source=all_data.csv, okres=-, region=kujawsko-pomorskie,type=-, value=116896.0\n",
            "- #2 dataset=TABLICA 1 (38). LICZBA OSÓB PODLEGAJĄCYCH UBEZPIECZENIU ZDROWOTNEMU WEDŁUG WOJEWÓDZTW, source=all_data.csv, okres=-, region=kujawsko-pomorskie,type=-, value=28347.0\n",
            "- #3 dataset=TABLICA 1 (38). LICZBA OSÓB PODLEGAJĄCYCH UBEZPIECZENIU ZDROWOTNEMU WEDŁUG WOJEWÓDZTW, source=all_data.csv, okres=-, region=kujawsko-pomorskie,type=-, value=124.0\n",
            "\n",
            "Q: ile wynosi przeciętny zasiłek macierzyński?\n",
            "1005,94 zł – przeciętny zasiłek macierzyński w Polsce w 2025 r. kw. I.\n",
            "\n",
            "[ŹRÓDŁA]\n",
            "- #1 dataset=TABLICA 9 (21). ZASIŁKI MACIERZYŃSKIE, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=1005.94\n",
            "- #2 dataset=TABLICA 9 (21). ZASIŁKI MACIERZYŃSKIE, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=23746.0\n",
            "- #3 dataset=TABLICA 9 (21). ZASIŁKI MACIERZYŃSKIE, source=all_data.csv, okres=2025-Q1, region=-,type=-, value=23886940.28\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ============================================================\n",
        "#  KRUS — RAG 3.1 (pod schemat: dataset, measure, value, region, period, typ)\n",
        "#  • CLEAN: value/period/measure/typ (nawias → typ, literówki)\n",
        "#  • TAGI: region/typ (synonimy) → metadane + wstrzyknięcie do tekstu\n",
        "#  • RETRIEVE: BM25 + Dense + MQ + HyDE → RRF → (opcjonalnie) Rerank (BGE m3)\n",
        "#  • REGUŁY: najnowszy okres + ogółem jeśli brak specyfiki\n",
        "#  • LLM: PLLuM-12B-chat (4-bit) — można wyłączyć i zwracać liczby bez LLM\n",
        "# ============================================================\n",
        "\n",
        "import os, re, json, unicodedata, random\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# --- LangChain\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.embeddings import Embeddings\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "# --- Cross-encoder (reranker) – opcjonalnie\n",
        "_HAS_RERANK = True\n",
        "try:\n",
        "    from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "    from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "except Exception:\n",
        "    _HAS_RERANK = False\n",
        "\n",
        "# --- Chroma (nowe/stare API)\n",
        "try:\n",
        "    from langchain_chroma import Chroma\n",
        "    _CHROMA_NEW = True\n",
        "except ImportError:\n",
        "    from langchain_community.vectorstores import Chroma\n",
        "    _CHROMA_NEW = False\n",
        "\n",
        "# -----------------------------\n",
        "# Ścieżki\n",
        "# -----------------------------\n",
        "CSV_DIR = \"C:/Users/admin/Desktop/KRUS-data-/KRUS-data-/all_dane\"  \n",
        "PERSIST_DIR = \"chroma_statystyki\"\n",
        "RERANKER_MODEL = \"radlab/polish-cross-encoder\"\n",
        "EMBEDDER_MODEL = \"intfloat/multilingual-e5-large\"\n",
        "# -----------------------------\n",
        "# Normalizacja / utils\n",
        "# -----------------------------\n",
        "def strip_acc(s: str) -> str:\n",
        "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", str(s)) if unicodedata.category(c) != \"Mn\")\n",
        "\n",
        "def norm_text(s: str) -> str:\n",
        "    s = strip_acc(str(s)).lower()\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def _serialize_tags(tags: List[str]) -> str:\n",
        "    return \",\".join(sorted({norm_text(t) for t in tags if t and str(t).strip()}))\n",
        "\n",
        "def _parse_tags_field(val) -> set[str]:\n",
        "    if val is None: return set()\n",
        "    if isinstance(val, list): return {norm_text(x) for x in val}\n",
        "    s = str(val).strip()\n",
        "    if s.startswith(\"[\") and s.endswith(\"]\"):\n",
        "        try:\n",
        "            arr = json.loads(s)\n",
        "            if isinstance(arr, list): return {norm_text(x) for x in arr}\n",
        "        except Exception:\n",
        "            pass\n",
        "    parts = re.split(r\"[,\\|;]\\s*\", s)\n",
        "    return {norm_text(p) for p in parts if p}\n",
        "\n",
        "# -----------------------------\n",
        "# Smart CSV reader (PL encodings)\n",
        "# -----------------------------\n",
        "def _read_csv_smart(path: str) -> pd.DataFrame:\n",
        "    for enc in [\"utf-8-sig\", \"cp1250\", \"iso-8859-2\", \"latin-1\"]:\n",
        "        try:\n",
        "            return pd.read_csv(path, encoding=enc)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "# -----------------------------\n",
        "# Synonimy regionów/typów\n",
        "# -----------------------------\n",
        "REGION_GENERAL = {norm_text(x) for x in [\"ogółem\",\"razem\",\"polska\",\"kraj\",\"ogolnie\",\"ogółem/razem\",\"cały kraj\",\"caly kraj\"]}\n",
        "REGION_SYNS = {\n",
        "    \"ogolem\": [\"ogółem\",\"razem\",\"polska\",\"kraj\",\"ogolnie\",\"cały kraj\",\"caly kraj\",\"ogółem/razem\"],\n",
        "}\n",
        "TYPE_SYNS = {\n",
        "    \"emerytura\": [\"emerytury\",\"świadczenia emerytalne\",\"swiadczenia emerytalne\",\"emerytalne\"],\n",
        "    \"renta_niezdolnosc\": [\"renty z tytułu niezdolności do pracy\",\"renta niezdolność\",\"renty_niezdolnosc_do_pracy\",\"niezdolnosc_do_pracy\"],\n",
        "    \"renta_rodzinna\": [\"renty rodzinne\",\"renta rodzinna\"],\n",
        "    \"swiadczenia_zabiegowe_robotnicze\": [\"świadczenia zabiegowe robotnicze\",\"swiadczenia zabiegowe robotnicze\"],\n",
        "}\n",
        "\n",
        "VOIVODESHIPS = [\n",
        "    \"dolnośląskie\",\"kujawsko-pomorskie\",\"lubelskie\",\"lubuskie\",\"łódzkie\",\n",
        "    \"małopolskie\",\"mazowieckie\",\"opolskie\",\"podkarpackie\",\"podlaskie\",\n",
        "    \"pomorskie\",\"śląskie\",\"świętokrzyskie\",\"warmińsko-mazurskie\",\n",
        "    \"wielkopolskie\",\"zachodniopomorskie\"\n",
        "]\n",
        "VOIV_NORM = {norm_text(v): v for v in VOIVODESHIPS}\n",
        "\n",
        "def expand_region_tags(val: Optional[str]) -> List[str]:\n",
        "    if val is None or str(val).strip()==\"\" or norm_text(val) in REGION_GENERAL:\n",
        "        return sorted(REGION_GENERAL)\n",
        "    v = norm_text(val)\n",
        "    tags = {v}\n",
        "    for syns in REGION_SYNS.values():\n",
        "        syns_n = {norm_text(s) for s in syns}\n",
        "        if v in syns_n: tags |= syns_n\n",
        "    return sorted(tags)\n",
        "\n",
        "def expand_type_tags(val: Optional[str]) -> List[str]:\n",
        "    if val is None or str(val).strip()==\"\":\n",
        "        return []\n",
        "    v = norm_text(val)\n",
        "    for k, syns in TYPE_SYNS.items():\n",
        "        syns_n = {norm_text(s) for s in syns}\n",
        "        if v==k or v in syns_n:\n",
        "            return [k] + sorted(syns_n)\n",
        "    return [v]\n",
        "\n",
        "# -----------------------------\n",
        "# Parsing liczb i okresów\n",
        "# -----------------------------\n",
        "def parse_value(x) -> Optional[float]:\n",
        "    if x is None: return None\n",
        "    s = str(x).strip()\n",
        "    if s == \"\" or norm_text(s) in {\"nan\", \"brak\", \"null\"}:\n",
        "        return None\n",
        "    s = s.replace(\" \", \"\").replace(\"\\u00A0\", \"\").replace(\",\", \".\")\n",
        "    try:\n",
        "        return float(s)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def norm_period(p: Optional[str]) -> Optional[str]:\n",
        "    if not p or str(p).strip()==\"\":\n",
        "        return None\n",
        "    s = str(p).replace(\"\\u00A0\",\" \").strip()\n",
        "    s = s.upper()\n",
        "    s = re.sub(r\"\\s+\", \"\", s)\n",
        "    s = s.replace(\"/\", \"-\").replace(\"_\",\"-\")\n",
        "    if re.match(r\"^20\\d{2}-Q[1-4]$\", s):\n",
        "        return s\n",
        "    m = re.match(r\"^(20\\d{2})Q([1-4])$\", s)\n",
        "    if m:\n",
        "        return f\"{m.group(1)}-Q{m.group(2)}\"\n",
        "    return s  # zostaw surowe (np. \"202-Q1\") — będzie mniej premiowane\n",
        "\n",
        "def clean_measure_and_type(measure: str, typ: Optional[str]) -> Tuple[str, Optional[str]]:\n",
        "    m = str(measure or \"\").strip()\n",
        "    t = None if (typ is None or str(typ).strip()==\"\") else str(typ).strip()\n",
        "    m = m.replace(\"przciętna\", \"przeciętna\")\n",
        "    m = m.replace(\" w zl\", \" w zł\")\n",
        "    paren = re.search(r\"\\(([^)]+)\\)\", m)\n",
        "    if paren and (t is None or t==\"\"):\n",
        "        t = paren.group(1).strip()\n",
        "    m = re.sub(r\"\\s*\\([^)]+\\)\\s*\", \" \", m)\n",
        "    m = re.sub(r\"\\s+\", \" \", m).strip()\n",
        "    return m, t\n",
        "\n",
        "def make_page_text(row: dict) -> str:\n",
        "    fields = [\n",
        "        (\"dataset\", row.get(\"dataset\")),\n",
        "        (\"measure\", row.get(\"measure_clean\")),\n",
        "        (\"value\", row.get(\"value_float\")),\n",
        "        (\"region\", row.get(\"region\") or \"-\"),\n",
        "        (\"period\", row.get(\"period_norm\") or row.get(\"period_raw\") or \"-\"),\n",
        "        (\"typ\", row.get(\"typ_clean\") or \"-\"),\n",
        "    ]\n",
        "    base = \" | \".join(f\"{k}: {v}\" for k, v in fields)\n",
        "    syn = []\n",
        "    if row.get(\"tags_region\"): syn.append(\"region_syn: \" + row[\"tags_region\"])\n",
        "    if row.get(\"tags_type\"):   syn.append(\"type_syn: \" + row[\"tags_type\"])\n",
        "    return base + (\" | \" + \" | \".join(syn) if syn else \"\")\n",
        "\n",
        "# -----------------------------\n",
        "# Budowa Document z CSV (schemat standardowy)\n",
        "# -----------------------------\n",
        "def build_documents_from_standard_csv(dataset_name: str, csv_path: str) -> List[Document]:\n",
        "    df = _read_csv_smart(csv_path)\n",
        "    required = [\"dataset\",\"measure\",\"value\",\"region\",\"period\",\"typ\"]\n",
        "    for c in required:\n",
        "        if c not in df.columns:\n",
        "            raise ValueError(f\"Brak wymaganej kolumny: {c}\")\n",
        "    docs: List[Document] = []\n",
        "    for i, row in df.iterrows():\n",
        "        dataset = str(row.get(\"dataset\") or \"\").strip()\n",
        "        measure_raw = row.get(\"measure\")\n",
        "        typ_raw     = row.get(\"typ\")\n",
        "        measure_clean, typ_clean = clean_measure_and_type(measure_raw, typ_raw)\n",
        "\n",
        "        value_float = parse_value(row.get(\"value\"))\n",
        "        region_raw  = None if pd.isna(row.get(\"region\")) else str(row.get(\"region\")).strip()\n",
        "        period_raw  = None if pd.isna(row.get(\"period\")) else str(row.get(\"period\")).strip()\n",
        "        period_norm_ = norm_period(period_raw)\n",
        "\n",
        "        tags_region = _serialize_tags(expand_region_tags(region_raw))\n",
        "        tags_type   = _serialize_tags(expand_type_tags(typ_clean))\n",
        "\n",
        "        rec = {\n",
        "            \"dataset\": dataset or dataset_name,\n",
        "            \"measure_clean\": measure_clean,\n",
        "            \"typ_clean\": typ_clean,\n",
        "            \"value_float\": value_float,\n",
        "            \"region\": region_raw,\n",
        "            \"period_raw\": period_raw,\n",
        "            \"period_norm\": period_norm_,\n",
        "            \"tags_region\": tags_region,\n",
        "            \"tags_type\": tags_type,\n",
        "        }\n",
        "        page_text = make_page_text(rec)\n",
        "        meta: Dict[str, Any] = {\n",
        "            \"dataset\": dataset or dataset_name,\n",
        "            \"source_file\": os.path.basename(csv_path),\n",
        "            \"row_index\": int(i),\n",
        "            \"okres\": period_norm_ or period_raw,\n",
        "            \"region\": region_raw,\n",
        "            \"type\": typ_clean,\n",
        "            \"measure\": measure_clean,\n",
        "            \"value\": value_float,\n",
        "            \"tags_region\": tags_region,\n",
        "            \"tags_type\": tags_type,\n",
        "        }\n",
        "        docs.append(Document(page_content=page_text, metadata=meta))\n",
        "    return docs\n",
        "\n",
        "# -----------------------------\n",
        "# EMBEDDINGS — MXBAI LARGE (1024d)\n",
        "# -----------------------------\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "class MXBAIEmbeddings(Embeddings):\n",
        "    def __init__(self, model_id: str = EMBEDDER_MODEL, device: Optional[str] = None):\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tok = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.net = AutoModel.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16 if self.device==\"cuda\" else torch.float32\n",
        "        ).to(self.device)\n",
        "        self.net.eval()\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def _encode(self, texts: List[str], is_query=False, batch_size=32) -> List[List[float]]:\n",
        "        prefix = \"query: \" if is_query else \"passage: \"\n",
        "        out = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = [prefix + t for t in texts[i:i+batch_size]]\n",
        "            enc = self.tok(batch, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
        "            last = self.net(**enc).last_hidden_state\n",
        "            attn = enc[\"attention_mask\"].unsqueeze(-1)\n",
        "            emb = (last * attn).sum(dim=1) / torch.clamp(attn.sum(dim=1), min=1)\n",
        "            emb = torch.nn.functional.normalize(emb, p=2, dim=1)\n",
        "            out.extend(emb.detach().cpu().tolist())\n",
        "        return out\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        return self._encode(texts, is_query=False)\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        return self._encode([text], is_query=True)[0]\n",
        "\n",
        "# -----------------------------\n",
        "# Załaduj wszystkie CSV z folderu\n",
        "#  • jeżeli mają dokładnie schemat standardowy → użyj build_documents_from_standard_csv\n",
        "#  • inaczej pomiń lub dopisz własny builder\n",
        "# -----------------------------\n",
        "def _looks_standard(df: pd.DataFrame) -> bool:\n",
        "    needed = {\"dataset\",\"measure\",\"value\",\"region\",\"period\",\"typ\"}\n",
        "    return needed.issubset(set(df.columns))\n",
        "\n",
        "CSV_SOURCES = {}\n",
        "if os.path.isdir(CSV_DIR):\n",
        "    for fn in os.listdir(CSV_DIR):\n",
        "        if fn.lower().endswith(\".csv\"):\n",
        "            CSV_SOURCES[os.path.splitext(fn)[0]] = os.path.join(CSV_DIR, fn)\n",
        "\n",
        "all_docs: List[Document] = []\n",
        "for name, path in CSV_SOURCES.items():\n",
        "    try:\n",
        "        df_head = _read_csv_smart(path).head(1)\n",
        "        if _looks_standard(df_head):\n",
        "            docs = build_documents_from_standard_csv(name, path)\n",
        "            all_docs.extend(docs)\n",
        "        else:\n",
        "            # Jeżeli masz inne pliki o innym schemacie — tu możesz dodać inny builder.\n",
        "            # Na razie je pomijamy, by uniknąć śmieci.\n",
        "            print(f\"[UWAGA] Pomijam '{path}' — niestandardowy schemat kolumn.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[BŁĄD] {path}: {e}\")\n",
        "\n",
        "print(f\"Zbudowano dokumentów: {len(all_docs)}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Wektorownia Chroma\n",
        "# -----------------------------\n",
        "emb = MXBAIEmbeddings()\n",
        "vectorstore = Chroma(\n",
        "    collection_name=\"statystyki\",\n",
        "    embedding_function=emb,\n",
        "    persist_directory=PERSIST_DIR\n",
        ")\n",
        "\n",
        "def _collection_count(vs) -> int:\n",
        "    coll = getattr(vs, \"_collection\", None)\n",
        "    try: return coll.count() if coll is not None else 0\n",
        "    except Exception: return 0\n",
        "\n",
        "if _collection_count(vectorstore) == 0 and all_docs:\n",
        "    vectorstore.add_documents(all_docs)\n",
        "    getattr(vectorstore, \"persist\", lambda: None)()\n",
        "\n",
        "# -----------------------------\n",
        "# BM25 retriever\n",
        "# -----------------------------\n",
        "bm25 = BM25Retriever.from_documents(all_docs)\n",
        "bm25.k = 80\n",
        "\n",
        "# -----------------------------\n",
        "# MultiQuery + HyDE (prosty, bez LLM)\n",
        "# -----------------------------\n",
        "def make_mq_prompts(q: str) -> List[str]:\n",
        "    qn = norm_text(q)\n",
        "    variants = {\n",
        "        f\"{q}\",\n",
        "        qn.replace(\"ile wynosi\", \"podaj wartość\"),\n",
        "        qn.replace(\"ile\", \"jaka jest liczba\"),\n",
        "        qn + \" (ogółem, Polska)\",\n",
        "        qn + \" (najnowszy okres)\",\n",
        "    }\n",
        "    return list(variants)\n",
        "\n",
        "def make_hyde(q: str) -> str:\n",
        "    qn = norm_text(q)\n",
        "    hints = []\n",
        "    if \"emery\" in qn: hints.append(\"świadczenia emerytalne\")\n",
        "    if \"renta\" in qn: hints.append(\"renty\")\n",
        "    if \"macierz\" in qn: hints.append(\"zasiłki macierzyńskie\")\n",
        "    if \"ubezpiec\" in qn: hints.append(\"ubezpieczeni\")\n",
        "    if \"płatnik\" in qn or \"platnik\" in qn: hints.append(\"płatnicy składek\")\n",
        "    if \"zdrowotn\" in qn: hints.append(\"ubezpieczenie zdrowotne\")\n",
        "    base = \"pseudo: \" + \" ; \".join(hints) if hints else \"pseudo: statystyki KRUS\"\n",
        "    return base + \" ; czas: najnowszy dostępny ; region: ogółem/Polska\"\n",
        "\n",
        "# -----------------------------\n",
        "# RRF — Reciprocal Rank Fusion\n",
        "# -----------------------------\n",
        "def rrf_merge(runs: List[List[Document]], k: int = 50, k_rrf: int = 60) -> List[Document]:\n",
        "    scores: Dict[Tuple[str,int], float] = {}\n",
        "    pos_maps: List[Dict[Tuple[str,int], int]] = []\n",
        "    for run in runs:\n",
        "        m = {}\n",
        "        for i, d in enumerate(run):\n",
        "            key = (d.metadata.get(\"source_file\",\"?\"), d.metadata.get(\"row_index\",-1))\n",
        "            m[key] = i\n",
        "        pos_maps.append(m)\n",
        "    keys = set().union(*[set(m.keys()) for m in pos_maps])\n",
        "    for key in keys:\n",
        "        score = 0.0\n",
        "        for m in pos_maps:\n",
        "            if key in m:\n",
        "                rank = m[key] + 1\n",
        "                score += 1.0 / (k_rrf + rank)\n",
        "        scores[key] = score\n",
        "    best_doc_for_key: Dict[Tuple[str,int], Document] = {}\n",
        "    for run in runs:\n",
        "        for d in run:\n",
        "            key = (d.metadata.get(\"source_file\",\"?\"), d.metadata.get(\"row_index\",-1))\n",
        "            if key not in best_doc_for_key:\n",
        "                best_doc_for_key[key] = d\n",
        "    merged = sorted(best_doc_for_key.values(),\n",
        "                    key=lambda d: scores[(d.metadata.get(\"source_file\",\"?\"), d.metadata.get(\"row_index\",-1))],\n",
        "                    reverse=True)\n",
        "    return merged[:k]\n",
        "\n",
        "# -----------------------------\n",
        "# Dense search helpers\n",
        "# -----------------------------\n",
        "def dense_search(query: str, k: int = 80) -> List[Document]:\n",
        "    retr = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
        "    return retr.invoke(query)\n",
        "\n",
        "def dense_search_on(texts: List[str], k: int = 40) -> List[Document]:\n",
        "    retr = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
        "    out = []\n",
        "    for t in texts:\n",
        "        out.extend(retr.invoke(t))\n",
        "    seen = set()\n",
        "    uniq = []\n",
        "    for d in out:\n",
        "        key = (d.metadata.get(\"source_file\",\"?\"), d.metadata.get(\"row_index\",-1))\n",
        "        if key in seen: continue\n",
        "        seen.add(key); uniq.append(d)\n",
        "    return uniq\n",
        "\n",
        "# -----------------------------\n",
        "# Reranker (opcjonalnie)\n",
        "# -----------------------------\n",
        "if _HAS_RERANK:\n",
        "    try:\n",
        "        cross_encoder = CrossEncoder(RERANKER_MODEL, device=device)\n",
        "        reranker = CrossEncoderReranker(model=cross_encoder, top_n=30)\n",
        "    except Exception:\n",
        "        _HAS_RERANK = False\n",
        "        reranker = None\n",
        "else:\n",
        "    reranker = None\n",
        "\n",
        "# -----------------------------\n",
        "# Okres → sortowanie\n",
        "# -----------------------------\n",
        "def period_key(p: Optional[str]) -> Tuple[int,int,int]:\n",
        "    if not p or str(p).strip()==\"\":\n",
        "        return (0,0,0)\n",
        "    s = str(p).lower().strip()\n",
        "    m = re.match(r\"^(20\\d{2})[-_/ ]?q([1-4])$\", s)\n",
        "    if m: return (int(m.group(1)), int(m.group(2)), 0)\n",
        "    m = re.match(r\"^(20\\d{2})[-_/](\\d{1,2})[-_/](\\d{1,2})$\", s)\n",
        "    if m:\n",
        "        y, mo, d = map(int, m.groups())\n",
        "        daynum = (mo-1)*31 + d\n",
        "        q = (mo-1)//3 + 1\n",
        "        return (y, q, daynum)\n",
        "    m = re.match(r\"^(20\\d{2})$\", s)\n",
        "    if m: return (int(m.group(1)), 0, 0)\n",
        "    return (0,0,0)\n",
        "\n",
        "# -----------------------------\n",
        "# Heurystyki zapytania: region/typ/najnowszy\n",
        "# -----------------------------\n",
        "REL_WORDS_NOW = {\"teraz\",\"obecnie\",\"w tym roku\",\"aktualnie\",\"bieżący\",\"biezacy\",\"najnowszy\",\"ostatni\",\"najświeższy\",\"nowszy\"}\n",
        "\n",
        "def parse_region(q: str) -> Optional[str]:\n",
        "    qn = norm_text(q)\n",
        "    for nn in VOIV_NORM.keys():\n",
        "        if nn in qn: return nn\n",
        "    m = re.search(r\"\\bw\\s+([a-ząćęłńóśźż\\- ]+?)(?:\\?|$|,|\\.)\", qn)\n",
        "    if m:\n",
        "        cand = norm_text(m.group(1).strip())\n",
        "        best, best_len = None, 0\n",
        "        for nn in VOIV_NORM.keys():\n",
        "            common = os.path.commonprefix([nn, cand])\n",
        "            if len(common) > best_len:\n",
        "                best, best_len = nn, len(common)\n",
        "        if best_len >= 5: return best\n",
        "    return None\n",
        "\n",
        "def parse_type(q: str) -> Optional[str]:\n",
        "    qn = norm_text(q)\n",
        "    best, best_len = None, 0\n",
        "    for k, syns in TYPE_SYNS.items():\n",
        "        for s in [k]+syns:\n",
        "            s_n = norm_text(s)\n",
        "            if s_n in qn and len(s_n) > best_len:\n",
        "                best, best_len = k, len(s_n)\n",
        "    return best\n",
        "\n",
        "def wants_latest(q: str) -> bool:\n",
        "    qn = norm_text(q)\n",
        "    if any(w in qn for w in REL_WORDS_NOW): return True\n",
        "    # jeśli nie wskazano innego okresu, preferuj najnowszy\n",
        "    return True\n",
        "\n",
        "# -----------------------------\n",
        "# SMART RETRIEVE\n",
        "# -----------------------------\n",
        "def retrieve(query: str, k_final: int = 24) -> List[Document]:\n",
        "    q = query\n",
        "\n",
        "    d0 = dense_search(q, k=80)\n",
        "    mq = make_mq_prompts(q);     d1 = dense_search_on(mq, k=40)\n",
        "    hy = make_hyde(q);           d2 = dense_search_on([hy], k=40)\n",
        "    bm25.k = 80;                 d3 = bm25.get_relevant_documents(q)\n",
        "\n",
        "    fused = rrf_merge([d0, d1, d2, d3], k=80, k_rrf=60)\n",
        "\n",
        "    if _HAS_RERANK and reranker is not None:\n",
        "        reranked = reranker.compress_documents(documents=fused, query=q)\n",
        "    else:\n",
        "        reranked = fused[:30]\n",
        "\n",
        "    region_req = parse_region(q)\n",
        "    type_req   = parse_type(q)\n",
        "    latest     = wants_latest(q)\n",
        "\n",
        "    pool = []\n",
        "    for d in reranked:\n",
        "        m = d.metadata or {}\n",
        "        reg = norm_text(m.get(\"region\")) if m.get(\"region\") else \"\"\n",
        "        typ = norm_text(m.get(\"type\")) if m.get(\"type\") else \"\"\n",
        "        tags_r = _parse_tags_field(m.get(\"tags_region\"))\n",
        "        tags_t = _parse_tags_field(m.get(\"tags_type\"))\n",
        "        if region_req:\n",
        "            rr = norm_text(region_req)\n",
        "            if (rr not in reg) and (rr not in tags_r):\n",
        "                continue\n",
        "        if type_req:\n",
        "            tt = norm_text(type_req)\n",
        "            if (tt not in typ) and (tt not in tags_t):\n",
        "                continue\n",
        "        pool.append(d)\n",
        "\n",
        "    if len(pool) < 10:\n",
        "        pool = reranked  # fallback gdy filtr zbyt agresywny\n",
        "\n",
        "    if latest:\n",
        "        def boost_general(d: Document) -> int:\n",
        "            tags_r = _parse_tags_field(d.metadata.get(\"tags_region\"))\n",
        "            return 1 if len(REGION_GENERAL.intersection(tags_r)) > 0 else 0\n",
        "        pool = sorted(\n",
        "            pool,\n",
        "            key=lambda d: (boost_general(d), period_key(d.metadata.get(\"okres\"))),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "    return pool[:k_final]\n",
        "\n",
        "# -----------------------------\n",
        "# LLM (PLLuM-12B-chat) — opcjonalnie\n",
        "# -----------------------------\n",
        "from transformers import AutoTokenizer as HF_AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "def build_pllum_llm(model_id: str = \"CYFRAGOVPL/PLLuM-12B-chat\", use_4bit: bool = True, max_new_tokens: int = 220):\n",
        "    if use_4bit:\n",
        "        bnb = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id, device_map=\"auto\", quantization_config=bnb, trust_remote_code=True\n",
        "        )\n",
        "    else:\n",
        "        llm_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", trust_remote_code=True)\n",
        "\n",
        "    tok = HF_AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "\n",
        "    gen_pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=llm_model,\n",
        "        tokenizer=tok,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.0,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tok.pad_token_id,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        return_full_text=False,\n",
        "    )\n",
        "    return HuggingFacePipeline(pipeline=gen_pipe)\n",
        "\n",
        "USE_LLM = True\n",
        "llm_pllum = build_pllum_llm() if USE_LLM else None\n",
        "\n",
        "def _trim(s: str, n: int = 500) -> str:\n",
        "    s = str(s)\n",
        "    return s if len(s) <= n else s[:n].rstrip() + \"…\"\n",
        "\n",
        "def format_context_for_llm(docs: List[Document], max_docs: int = 6, max_snip_chars: int = 420) -> str:\n",
        "    lines = []\n",
        "    for i, d in enumerate(docs[:max_docs], 1):\n",
        "        meta = d.metadata or {}\n",
        "        okres  = meta.get(\"okres\", \"\")\n",
        "        region = meta.get(\"region\", \"\")\n",
        "        dataset = meta.get(\"dataset\", \"\")\n",
        "        source  = meta.get(\"source_file\", \"\")\n",
        "        rowidx  = meta.get(\"row_index\", \"\")\n",
        "        header = f\"[{i}] dataset={dataset} | source={source} | okres={okres or '-'} | region={region or '-'} | row={rowidx}\"\n",
        "        body = _trim(d.page_content, max_snip_chars)\n",
        "        lines.append(header + \"\\n\" + body)\n",
        "    return \"\\n\\n\".join(lines)\n",
        "\n",
        "SYS_MSG = (\n",
        "    \"Jesteś asystentem danych KRUS. Odpowiadasz zwięźle, liczbowo i WYŁĄCZNIE na podstawie podanych fragmentów.\\n\"\n",
        "    \"Zasady:\\n\"\n",
        "    \"• Jeśli pytanie nie precyzuje daty/czasu ⇒ wybierz NAJNOWSZY okres w danych czyli z rokiem 2025 lub w formacie XX.XX.2025.\\n\"\n",
        "    \"• Jeśli brak województwa/typu ⇒ preferuj rekordy ogólne (Ogółem/Polska).\\n\"\n",
        "    \"• Gdy w danych brakuje odpowiedzi ⇒ powiedz wprost, że brak informacji.\\n\"\n",
        "    \"• W odpowiedzi NIE dodawaj źródeł ani numerów fragmentów.\"\n",
        ")\n",
        "\n",
        "USER_TEMPLATE = (\n",
        "    \"<PYTANIE>\\n{question}\\n</PYTANIE>\\n\\n\"\n",
        "    \"<FRAGMENTY>\\n{context}\\n</FRAGMENTY>\\n\\n\"\n",
        "    \"Sformułuj 1–2 zdaniową odpowiedź po polsku, podaj tylko liczby/jednostki i okres/region. \"\n",
        "    \"Nie dodawaj źródeł ani numerów fragmentów.\"\n",
        ")\n",
        "\n",
        "def build_prompt(question: str, context: str) -> str:\n",
        "    return f\"<s>[INST] <<SYS>>{SYS_MSG}<</SYS>>\\n\" + USER_TEMPLATE.format(question=question, context=context) + \"[/INST]\"\n",
        "\n",
        "def answer_pllum(question: str, k_ctx: int = 8) -> str:\n",
        "    # Pobierz dokumenty z retrieva\n",
        "    docs = retrieve(question, k_final=max(12, k_ctx))\n",
        "    if not docs:\n",
        "        return \"Brak danych pasujących do pytania.\\n[ŹRÓDŁA]\\n– brak\"\n",
        "\n",
        "    # Analizuj pytanie, aby lepiej filtrować dokumenty\n",
        "    region_req = parse_region(question)\n",
        "    type_req = parse_type(question)\n",
        "    latest = wants_latest(question)\n",
        "    \n",
        "    # Najpierw znajdź najlepiej dopasowane dokumenty według kryteriów\n",
        "    scored_docs = []\n",
        "    for doc in docs:\n",
        "        score = 0\n",
        "        mv = doc.metadata\n",
        "        \n",
        "        # Punkty za dopasowanie regionu\n",
        "        if region_req:\n",
        "            doc_region = norm_text(mv.get(\"region\", \"\"))\n",
        "            tags_r = _parse_tags_field(mv.get(\"tags_region\"))\n",
        "            region_norm = norm_text(region_req)\n",
        "            if region_norm in doc_region or region_norm in tags_r:\n",
        "                score += 10\n",
        "        else:\n",
        "            # Jeśli nie określono regionu, preferuj ogółem\n",
        "            tags_r = _parse_tags_field(mv.get(\"tags_region\"))\n",
        "            if len(REGION_GENERAL.intersection(tags_r)) > 0:\n",
        "                score += 5\n",
        "        \n",
        "        # Punkty za dopasowanie typu\n",
        "        if type_req:\n",
        "            doc_type = norm_text(mv.get(\"type\", \"\"))\n",
        "            tags_t = _parse_tags_field(mv.get(\"tags_type\"))\n",
        "            type_norm = norm_text(type_req)\n",
        "            if type_norm in doc_type or type_norm in tags_t:\n",
        "                score += 10\n",
        "        \n",
        "        # Punkty za najnowszy okres\n",
        "        if latest:\n",
        "            period_score = period_key(mv.get(\"okres\"))\n",
        "            score += period_score[0] * 1000 + period_score[1] * 10 + period_score[2]\n",
        "        \n",
        "        # Punkty za obecność wartości numerycznej\n",
        "        if mv.get(\"value\") is not None:\n",
        "            score += 3\n",
        "            \n",
        "        scored_docs.append((score, doc))\n",
        "    \n",
        "    # Sortuj według score (najlepsze pierwsze)\n",
        "    scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
        "    best_docs = [doc for score, doc in scored_docs]\n",
        "    \n",
        "    # Wybierz dokumenty do kontekstu LLM - najlepiej dopasowane\n",
        "    context_docs = best_docs[:k_ctx]\n",
        "    \n",
        "    # Przygotuj kontekst dla LLM\n",
        "    ctx = format_context_for_llm(context_docs, max_docs=len(context_docs))\n",
        "\n",
        "    # --- Tryb bez LLM: zwróć top 1 rekord ---\n",
        "    if not USE_LLM:\n",
        "        top = best_docs[0]\n",
        "        mv = top.metadata\n",
        "        return (\n",
        "            f\"{mv.get('value')} (okres: {mv.get('okres') or '-'}, \"\n",
        "            f\"region: {mv.get('region') or 'ogółem'}, \"\n",
        "            f\"measure: {mv.get('measure')})\\n\\n\"\n",
        "            f\"[ŹRÓDŁA]\\n- #1 dataset={mv.get('dataset')} | source={mv.get('source_file')} | \"\n",
        "            f\"okres={mv.get('okres')} | region={mv.get('region') or '-'}\"\n",
        "        )\n",
        "\n",
        "    # --- Tryb z LLM ---\n",
        "    prompt = build_prompt(question, ctx)\n",
        "    try:\n",
        "        raw_answer = llm_pllum.invoke(prompt)\n",
        "    except Exception as e:\n",
        "        headers_only = \"\\n\".join(line.split(\"\\n\", 1)[0] for line in ctx.split(\"\\n\\n\"))\n",
        "        return f\"Nie udało się wygenerować odpowiedzi LLM ({e}).\\nDostępny kontekst:\\n{headers_only}\"\n",
        "\n",
        "    # --- Znajdź dokumenty, które najprawdopodobniej były używane do odpowiedzi ---\n",
        "    # Weź dokumenty użyte w kontekście LLM (te same, które model widział)\n",
        "    used_docs = context_docs[:3]  # Pokaż max 3 najlepiej dopasowane\n",
        "    \n",
        "    # Alternatywnie: można też filtrować na podstawie słów kluczowych z odpowiedzi\n",
        "    answer_words = set(norm_text(raw_answer).split())\n",
        "    if len(answer_words) > 3:  # Jeśli odpowiedź ma jakąś treść\n",
        "        # Znajdź dokumenty, które mają największe pokrycie słów z odpowiedzią\n",
        "        word_matched_docs = []\n",
        "        for doc in context_docs:\n",
        "            doc_words = set(norm_text(doc.page_content).split())\n",
        "            overlap = len(answer_words.intersection(doc_words))\n",
        "            if overlap > 0:\n",
        "                word_matched_docs.append((overlap, doc))\n",
        "        \n",
        "        if word_matched_docs:\n",
        "            word_matched_docs.sort(key=lambda x: x[0], reverse=True)\n",
        "            used_docs = [doc for overlap, doc in word_matched_docs[:3]]\n",
        "\n",
        "    # --- Budowanie sekcji źródeł z rzeczywiście użytych dokumentów ---\n",
        "    sources = []\n",
        "    for i, doc in enumerate(used_docs, start=1):\n",
        "        mv = doc.metadata\n",
        "        # Dodaj wartość do opisu źródła, jeśli dostępna\n",
        "        value_str = f\", value={mv.get('value', '-')}\" if mv.get('value') is not None else \"\"\n",
        "        sources.append(\n",
        "            f\"- #{i} dataset={mv.get('dataset', '-')}, \"\n",
        "            f\"source={mv.get('source_file', '-')}, \"\n",
        "            f\"okres={mv.get('okres', '-')}, \"\n",
        "            f\"region={mv.get('region', 'ogółem')}, \"\n",
        "            f\"type={mv.get('type', '-')}\"\n",
        "            f\"{value_str}\"\n",
        "        )\n",
        "\n",
        "    # Sklejanie odpowiedzi + źródeł\n",
        "    final_answer = raw_answer.strip() + \"\\n\\n[ŹRÓDŁA]\\n\" + \"\\n\".join(sources)\n",
        "    return final_answer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AazS6q0u8rW",
        "outputId": "7aa4515b-a5ea-4ad9-f738-4357a8c916e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Q: ile zlozono wnioskow o przyznanie emerytury?\n",
            "W pierwszym kwartale 2025 roku złożono 8915 wniosków o przyznanie emerytury.\n",
            "\n",
            "[ŹRÓDŁA]\n",
            "- #1 dataset=WNIOSKI O PRZYZNANIE EMERYTUR I RENT WEDŁUG RODZAJÓW, source=all_data.csv, okres=2025-Q1, region=-type=-, value=8915.0\n",
            "- #2 dataset=WNIOSKI O PRZYZNANIE EMERYTUR I RENT WEDŁUG RODZAJÓW, source=all_data.csv, okres=2025-Q1, region=-type=-, value=10257.0\n",
            "- #3 dataset=WNIOSKI O PRZYZNANIE EMERYTUR I RENT WEDŁUG RODZAJÓW, source=all_data.csv, okres=2025-Q1, region=-type=-, value=1678.0\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    tests = [\n",
        "        \"ile zlozono wnioskow o przyznanie emerytury?\",\n",
        "    ]\n",
        "    for q in tests:\n",
        "        print(\"\\nQ:\", q)\n",
        "        print(answer_pllum(q, k_ctx=8))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3cd7831925aa47d296beadb56b1ade3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "472faff74a1a4543826b27f2b741ab6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "650da3fb714447d5a3cd696b19420f8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b476dac061b439cab135a86e7130a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_650da3fb714447d5a3cd696b19420f8c",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_472faff74a1a4543826b27f2b741ab6f",
            "value": 5
          }
        },
        "74d9f30307f84463b4edf92c670fef64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe823240fc4848cf8e89cd9c24550482",
              "IPY_MODEL_6b476dac061b439cab135a86e7130a8e",
              "IPY_MODEL_77e6d4fffcc840669bceeaa7d2cb3c44"
            ],
            "layout": "IPY_MODEL_b28a3cdb351442eabaf28613442a9ed3"
          }
        },
        "77e6d4fffcc840669bceeaa7d2cb3c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb048b7b99f74b5ca2fee9f57b1bdddc",
            "placeholder": "​",
            "style": "IPY_MODEL_3cd7831925aa47d296beadb56b1ade3e",
            "value": " 5/5 [01:17&lt;00:00, 16.72s/it]"
          }
        },
        "83b3c291a24a4926b4917b2d07a5bd39": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8754bc98ba9d41e292b632655bfd312d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b28a3cdb351442eabaf28613442a9ed3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb048b7b99f74b5ca2fee9f57b1bdddc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe823240fc4848cf8e89cd9c24550482": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83b3c291a24a4926b4917b2d07a5bd39",
            "placeholder": "​",
            "style": "IPY_MODEL_8754bc98ba9d41e292b632655bfd312d",
            "value": "Loading checkpoint shards: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
